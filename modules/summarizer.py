# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ìˆ˜í–‰
import os, sys, json

# í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë“ˆ
from sklearn.feature_extraction.text import TfidfVectorizer # TfidVectorization: TF-IDF ë²¡í„°í™”ë¥¼ ìˆ˜í–‰í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ

# í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ ëª¨ë“ˆ
from konlpy.tag import Okt # í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ê¸° (ëª…ì‚¬ ì¶”ì¶œ ìˆ˜í–‰)

# í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ data í´ë”ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))  # í˜„ì¬ íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬
parent_dir = os.path.dirname(current_dir)  # ìƒìœ„ ë””ë ‰í† ë¦¬ (í”„ë¡œì íŠ¸ ë£¨íŠ¸)
data_path = os.path.join(parent_dir, "data")  # data í´ë” ê²½ë¡œ
model_path = os.path.join(parent_dir, "models") # model í´ë” ê²½ë¡œë¡œ
json_path = os.path.join(parent_dir, "json", "news_data.json")  # JSON íŒŒì¼ ê²½ë¡œ

sys.path.append(data_path)  # data í´ë”ë¥¼ Python ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œì— ì¶”ê°€
sys.path.append(model_path) # model í´ë”ë¥¼ aPython ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œì— ì¶”ê°€

# í”„ë¡œì íŠ¸ ë‚´ ë°ì´í„° ëª¨ë“ˆ
from sample_news import sample_news  # ìƒ˜í”Œ ë‰´ìŠ¤ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
from stopwords import STOPWORDS  # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°

# JSON íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
def load_json(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)  # JSON íŒŒì¼ì„ íŒŒì´ì¬ ê°ì²´ë¡œ ë³€í™˜
    return data

# í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„±
okt = Okt()

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
def preprocess_text(text):
    text = re.sub(r"[^ê°€-í£\s]", "", text)  # í•œê¸€ê³¼ ê³µë°± ì´ì™¸ì˜ ë¬¸ì ì œê±°
    words = okt.nouns(text)  # ëª…ì‚¬ ì¶”ì¶œ

    # ë¶ˆìš©ì–´ ë° í•œ ê¸€ì ë‹¨ì–´ ì œê±°
    filtered_words = [word for word in words if len(word) > 1 and word not in STOPWORDS]

    return " ".join(filtered_words)  # ê³µë°±ìœ¼ë¡œ ì—°ê²°ëœ ë¬¸ìì—´ ë°˜í™˜ (TF-IDFì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•¨)

# TF-IDFë¥¼ ì´ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(
    text, # ì…ë ¥ í…ŒìŠ¤íŠ¸ (ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸)
    top_n=10 # ìƒìœ„ ëª‡ ê°œì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí• ì§€ ê²°ì •
):
    processed_text = preprocess_text(text)  # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

    # ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™” ë° TF-IDF ë²¡í„° ë³€í™˜
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([processed_text])
    
    # ê° ë‹¨ì–´ì˜ TF-IDF ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    feature_names = vectorizer.get_feature_names_out()
    scores = tfidf_matrix.toarray()[0]

    # TF-IDF ì ìˆ˜ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    keyword_scores = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)

    # ìƒìœ„ top_nê°œì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = [word for word, score in keyword_scores[:top_n]]

    return keywords # ì¤‘ìš” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

# ìƒ˜í”Œ ë‰´ìŠ¤ ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
# keywords = extract_keywords(sample_news.content, top_n=10)

# # ê²°ê³¼ ì¶œë ¥
# print("ğŸ”¥ ì¤‘ìš” í‚¤ì›Œë“œ (TF-IDF ê¸°ì¤€ ìƒìœ„ 10ê°œ):")
# print(keywords)

# JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
news_data = load_json(json_path)

# ê° ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœëŒ€ 5ê°œ ë‰´ìŠ¤ë§Œ í…ŒìŠ¤íŠ¸)
for i, doc in enumerate(news_data):
    keywords = extract_keywords(doc["content"], top_n=15)
    print(f"\nğŸ”¥ ë‰´ìŠ¤ ì¤‘ìš” í‚¤ì›Œë“œë“œ {i+1} í‚¤ì›Œë“œ: {keywords}")


